{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFM practice\n",
    "\n",
    "Ref:\n",
    "- https://github.com/princewen/tensorflow_practice/blob/master/recommendation/recommendation-FFM-Demo/FFM_model.py\n",
    "- https://www.jianshu.com/p/781cde3d5f3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x_size = 20  # 每个样本输入的维度（20个特征，10个属于field1，10个属于field2）\n",
    "field_size = 2\n",
    "\n",
    "vector_dimension = 3\n",
    "\n",
    "total_plan_train_steps = 1000\n",
    "# 使用SGD，每一个样本进行依次梯度下降，更新参数\n",
    "batch_size = 1\n",
    "\n",
    "all_data_size = 1000  # 训练样本个数\n",
    "\n",
    "lr = 0.01\n",
    "\n",
    "MODEL_SAVE_PATH = \"TFModel\"\n",
    "MODEL_NAME = \"FFM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机生成一些模拟数据\n",
    "def gen_data():\n",
    "    labels = [-1,1]\n",
    "    y = [np.random.choice(labels,1)[0] for _ in range(all_data_size)]\n",
    "    x_field = [i // 10 for i in range(input_x_size)]  # 前10个值为0，后10个值为1\n",
    "    x = np.random.randint(0,2,size=(all_data_size,input_x_size))\n",
    "    return x,y,x_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义权重项（bias项的权重，一维特征的权重，交叉特征的权重\n",
    "\n",
    "# 交叉特征的权重（ 每个v的长度为vector_dimension维，共有input_x_size * field_size 个v向量\n",
    "def createTwoDimensionWeight(input_x_size,field_size,vector_dimension):\n",
    "    weights = tf.truncated_normal([input_x_size,field_size,vector_dimension])\n",
    "\n",
    "    tf_weights = tf.Variable(weights)\n",
    "\n",
    "    return tf_weights\n",
    "\n",
    "def createOneDimensionWeight(input_x_size):\n",
    "    weights = tf.truncated_normal([input_x_size])\n",
    "    tf_weights = tf.Variable(weights)\n",
    "    return tf_weights\n",
    "\n",
    "def createZeroDimensionWeight():\n",
    "    weights = tf.truncated_normal([1])\n",
    "    tf_weights = tf.Variable(weights)\n",
    "    return tf_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ffm](https://codingcat.cn/uploads/images/4f60aed0-22d7-11e9-9b7d-fa163e7a698c.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据ffm的公式，来计算估计值，对照公式看很容易理解\n",
    "\n",
    "# 重点学习一下这个方法里面用到的tf的内置方法及作用\n",
    "def inference(input_x,input_x_field,zeroWeights,oneDimWeights,thirdWeight):\n",
    "    \"\"\"计算回归模型输出的值\"\"\"\n",
    "\n",
    "    secondValue = tf.reduce_sum(tf.multiply(oneDimWeights,input_x,name='secondValue'))\n",
    "\n",
    "    firstTwoValue = tf.add(zeroWeights, secondValue, name=\"firstTwoValue\")\n",
    "\n",
    "    thirdValue = tf.Variable(0.0,dtype=tf.float32)\n",
    "    input_shape = input_x_size\n",
    "\n",
    "    for i in range(input_shape):\n",
    "        featureIndex1 = i\n",
    "        fieldIndex1 = int(input_x_field[i]) # 该feature所属field\n",
    "        for j in range(i+1,input_shape):\n",
    "            featureIndex2 = j\n",
    "            fieldIndex2 = int(input_x_field[j]) # 该feature所属field\n",
    "            # convert_to_tensor 作用：将python数据转换为tf的tensor\n",
    "            vectorLeft = tf.convert_to_tensor([[featureIndex1,fieldIndex2,i] for i in range(vector_dimension)])\n",
    "            # gather_nd 作用：根据索引，从向量中把索引位置的值提取出来，组成新的向量\n",
    "            weightLeft = tf.gather_nd(thirdWeight,vectorLeft)\n",
    "            # squeeze 作用：删除所有长度为1的维度，例如某个三维向量各维度长度分别为[1,2,3] squeeze后得到的向量，维度为[2,3]\n",
    "            # 此处的squeeze相当于把一个[1,1,3]的三维挤成一个1维向量 [3]\n",
    "            weightLeftAfterCut = tf.squeeze(weightLeft)\n",
    "\n",
    "            vectorRight = tf.convert_to_tensor([[featureIndex2,fieldIndex1,i] for i in range(vector_dimension)])\n",
    "            weightRight = tf.gather_nd(thirdWeight,vectorRight)\n",
    "            weightRightAfterCut = tf.squeeze(weightRight)\n",
    "            \n",
    "            # reduce_sum 压缩求和，例如 reduct_sum([[1,1],[2,3]]) = 1+1+2+3 = 7\n",
    "            tempValue = tf.reduce_sum(tf.multiply(weightLeftAfterCut,weightRightAfterCut))\n",
    "\n",
    "            indices2 = [i]\n",
    "            indices3 = [j]\n",
    "\n",
    "            xi = tf.squeeze(tf.gather_nd(input_x, indices2))\n",
    "            xj = tf.squeeze(tf.gather_nd(input_x, indices3))\n",
    "\n",
    "            product = tf.reduce_sum(tf.multiply(xi, xj))\n",
    "\n",
    "            secondItemVal = tf.multiply(tempValue, product)\n",
    "\n",
    "            tf.assign(thirdValue, tf.add(thirdValue, secondItemVal))\n",
    "\n",
    "    return tf.add(firstTwoValue,thirdValue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "\n",
    "global_step = tf.Variable(0,trainable=False)\n",
    "# 生成模拟数据\n",
    "trainx,trainy,trainx_field = gen_data()\n",
    "\n",
    "# 定义输出placeholder\n",
    "input_x = tf.placeholder(tf.float32,[input_x_size ])\n",
    "input_y = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "lambda_w = tf.constant(0.001, name='lambda_w')\n",
    "lambda_v = tf.constant(0.001, name='lambda_v')\n",
    "\n",
    "# 初始化权重参数\n",
    "zeroWeights = createZeroDimensionWeight()\n",
    "\n",
    "oneDimWeights = createOneDimensionWeight(input_x_size)\n",
    "\n",
    "thirdWeight = createTwoDimensionWeight(input_x_size,  # 创建二次项的权重变量\n",
    "                                       field_size,\n",
    "                                       vector_dimension)  # n * f * k\n",
    "\n",
    "# 计算得到 y_\n",
    "y_ = inference(input_x, trainx_field,zeroWeights,oneDimWeights,thirdWeight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数及正则化项\n",
    "l2_norm = tf.reduce_sum(\n",
    "    tf.add(\n",
    "        tf.multiply(lambda_w, tf.pow(oneDimWeights, 2)),\n",
    "        tf.reduce_sum(tf.multiply(lambda_v, tf.pow(thirdWeight, 2)),axis=[1,2])\n",
    "    )\n",
    ")\n",
    "\n",
    "loss = tf.log(1 + tf.exp(input_y * y_)) + l2_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义train step\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [0.71496004] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [1.0718213] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [0.47387955] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [1.738892] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [0.58538455] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [0.8071034] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [3.3097954] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [1.0471672] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [0.90875643] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [2.7981732] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [3.7000327] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [2.1686242] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [0.90774184] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [0.29410347] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [1.5031416] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [0.9635091] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [1.2312484] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [1.7995532] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [0.47299284] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [0.30336133] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [0.69651747] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [2.9754164] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [0.3262328] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [0.16241528] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [2.5145204] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [0.1992968] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [0.92013055] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [0.34788665] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [0.14587072] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [1.2137175] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [0.1375728] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [0.22648245] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [4.655556] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [1.630115] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [2.4509885] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [2.1686275] \n",
      "After  0 training   step(s)   ,   loss    on    training    batch   is  [1.4117289] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1e426aac752a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_SAVE_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_SAVE_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/summary/writer/writer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logdir, graph, max_queue, flush_secs, graph_def, filename_suffix, session)\u001b[0m\n\u001b[1;32m    365\u001b[0m       event_writer = EventFileWriter(logdir, max_queue, flush_secs,\n\u001b[1;32m    366\u001b[0m                                      filename_suffix)\n\u001b[0;32m--> 367\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_writer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/summary/writer/writer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, event_writer, graph, graph_def)\u001b[0m\n\u001b[1;32m     88\u001b[0m       self.add_meta_graph(\n\u001b[1;32m     89\u001b[0m           meta_graph.create_meta_graph_def(graph_def=graph_def or\n\u001b[0;32m---> 90\u001b[0;31m                                            maybe_graph_as_def))\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# This set contains tags of Summary Values that have been encountered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mcreate_meta_graph_def\u001b[0;34m(meta_info_def, graph_def, saver_def, collection_list, graph, export_scope, exclude_nodes, clear_extraneous_savers, strip_default_attrs)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m   \u001b[0;31m# Fills in meta_info_def.stripped_op_list using the ops from graph_def.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(total_plan_train_steps):\n",
    "        for t in range(all_data_size):\n",
    "            input_x_batch = trainx[t]\n",
    "            input_y_batch = trainy[t]\n",
    "            predict_loss,_, steps = sess.run([loss,train_step, global_step],\n",
    "                                           feed_dict={input_x: input_x_batch, input_y: input_y_batch})\n",
    "\n",
    "            print(\"After  {step} training   step(s)   ,   loss    on    training    batch   is  {predict_loss} \"\n",
    "                  .format(step=steps, predict_loss=predict_loss))\n",
    "\n",
    "            saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=steps)\n",
    "            writer = tf.summary.FileWriter(os.path.join(MODEL_SAVE_PATH, MODEL_NAME), tf.get_default_graph())\n",
    "            writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
